{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du Dataset\n",
    "\n",
    "#### Partie 3 - Constructions de l'ensemble des fichiers et dossiers pour l'entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>**Projet M1 - Détection d'anomalies sur imagerie médicale - Dataset creation**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version              : 2\n",
      "Notebook id          : Dataset creation\n",
      "Run time             : Thursday 16 February 2023, 18:33:15\n",
      "Hostname             : ludovic-ubu (Linux)\n",
      "Tensorflow log level : Info + Warning + Error  (=0)\n",
      "Datasets dir         : /media/ludovic/Cache/ENID\n",
      "Run dir              : ./run\n",
      "Update keras cache   : False\n",
      "numpy                : 1.21.5\n",
      "skimage              : 0.19.2\n",
      "matplotlib           : 3.5.1\n",
      "pandas               : 1.3.5\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append('..')\n",
    "import Modules.utils as utils\n",
    "import Modules.init as init\n",
    "import Modules.filters as filter\n",
    "\n",
    "datasets_dir = init.init('Dataset creation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer l'arborecence suivante (avec la proportion de nos fichiers): \n",
    "- Train (70 %)\n",
    "  - Images\n",
    "  - Masks\n",
    "- Test (20 %)\n",
    "  - Images\n",
    "  - Masks\n",
    "- Validation (10 %)\n",
    "  - Images\n",
    "  - Masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de l'arborescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arborescence générée avec succès.\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.join(datasets_dir, 'DATASET')\n",
    "directories = ['Train', 'Test', 'Validation']\n",
    "sub_directories = ['Images', 'Masks']\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "\n",
    "for directory in directories:\n",
    "    dir_path = os.path.join(root_dir, directory)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    \n",
    "    for sub_directory in sub_directories:\n",
    "        sub_dir_path = os.path.join(dir_path, sub_directory)\n",
    "        if not os.path.exists(sub_dir_path):\n",
    "            os.mkdir(sub_dir_path)\n",
    "        img_dir_path = os.path.join(sub_dir_path, 'img')\n",
    "        if not os.path.exists(img_dir_path):\n",
    "            os.mkdir(img_dir_path)\n",
    "\n",
    "print(\"Arborescence générée avec succès.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent créer les images qui seront répartis comme définis ci-dessus dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images total :  12444 \t\t\tSoit : 100 %\n",
      "Nombre d'images d'entrainement :  8710 \t\tSoit : 69.99  %\n",
      "Nombre d'images de test :  2490 \t\tSoit : 20.01  %\n",
      "Nombre d'images de validation :  1244 \t\tSoit : 10.00  %\n"
     ]
    }
   ],
   "source": [
    "# On mélange les lignes de notre fichier et on récupères 3 dataframes\n",
    "df = pd.read_csv('run/dataset_final.csv', header=0)\n",
    "\n",
    "train_df, test_df, validate_df = utils.train_validate_test_split(df, train_percent=0.7, validate_percent=0.1, seed=40)\n",
    "\n",
    "print(\"Nombre d'images total : \", len(df), \"\\t\\t\\tSoit : 100 %\")\n",
    "print(\"Nombre d'images d'entrainement : \", len(train_df), \"\\t\\tSoit :\",\"{:.2f}\".format((len(train_df)*100)/len(df)), \" %\")\n",
    "print(\"Nombre d'images de test : \", len(test_df), \"\\t\\tSoit :\",\"{:.2f}\".format((len(test_df)*100)/len(df)), \" %\")\n",
    "print(\"Nombre d'images de validation : \", len(validate_df), \"\\t\\tSoit :\",\"{:.2f}\".format((len(validate_df)*100)/len(df)), \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque dataframe, nous allons itérer chaque couple de fichier (image + masque) et les répartir dans les sous dossiers correspondant. Nous effecturons le traitement de chaque images ainsi que le redimensionnement si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(datasets_dir, 'DATASET')\n",
    "\n",
    "dataframe_list = [train_df, test_df, validate_df]\n",
    "folder_list = ['Train', 'Test', 'Validation']\n",
    "sub_folder_list = ['Images', 'Masks']\n",
    "\n",
    "def process_case(case):\n",
    "    folder_path = os.path.join(path, folder_list[case])\n",
    "    img_path = os.path.join(folder_path, sub_folder_list[0], 'img')\n",
    "    mask_path = os.path.join(folder_path, sub_folder_list[1], 'img')\n",
    "    nb_elem = len(dataframe_list[case])\n",
    "    for i in range(nb_elem):\n",
    "        utils.progressBar(i, nb_elem, prefix = 'Traitement dataset ' + str(folder_list[case]))\n",
    "        image_array, mask_array = utils.getImgMsk(dataframe_list[case], i)\n",
    "        img_stretched = filter.hist_stretching(image_array)\n",
    "        img_sharped = filter.sharpening(img_stretched, 5, 2.5)\n",
    "        img_sharped, mask_array = utils.resize_img_mask(384, 640, img_sharped, mask_array)\n",
    "        mask_array[mask_array == 118] = 1\n",
    "        \n",
    "        final_img = Image.fromarray(img_sharped)\n",
    "        final_mask = Image.fromarray(mask_array)\n",
    "        \n",
    "        final_img.save(os.path.join(img_path, str(i) + \".jpeg\"))\n",
    "        final_mask.save(os.path.join(mask_path, str(i) + \".png\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = [executor.submit(process_case, case) for case in range(3)]\n",
    "    for f in concurrent.futures.as_completed(results):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6ca67135e7c0bd86858f2589a46579e62439356507fe560aa94f9e6dda9695e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
